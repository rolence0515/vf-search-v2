import os
import csv
import psycopg2
import urllib.parse
import torch
import numpy as np
from transformers import AutoModel, AutoTokenizer

# ä½ çš„ GCP PostgreSQL è³‡æ–™åº«é€£ç·šè³‡è¨Š
# åŸå§‹é€£ç·šè³‡è¨Š
username = "rolence"
password = "zaq1@#$%^&*"  # é€™è£¡æœ‰ç‰¹æ®Šå­—å…ƒ
host = "34.84.63.151"
port = "5432"
database = "postgres"

# é€²è¡Œ URL ç·¨ç¢¼
encoded_password = urllib.parse.quote_plus(password)

# é‡æ–°çµ„åˆæ­£ç¢ºçš„ DATABASE_URL
DATABASE_URL = f"postgresql://{username}:{encoded_password}@{host}:{port}/{database}"

# ä½¿ç”¨ BERT-base-chinese å’Œ BERT-base-multilingual-cased ä½œç‚º Embedding Model
MODEL_NAME_1 = "bert-base-chinese"
MODEL_NAME_2 = "bert-base-multilingual-cased"
tokenizer_1 = AutoTokenizer.from_pretrained(MODEL_NAME_1)
model_1 = AutoModel.from_pretrained(MODEL_NAME_1)
tokenizer_2 = AutoTokenizer.from_pretrained(MODEL_NAME_2)
model_2 = AutoModel.from_pretrained(MODEL_NAME_2)

# è½‰æ›æ–‡å­—ç‚ºå‘é‡
def text_to_embedding(text):
    tokens_1 = tokenizer_1(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    tokens_2 = tokenizer_2(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        output_1 = model_1(**tokens_1)
        output_2 = model_2(**tokens_2)
    embedding_1 = output_1.last_hidden_state[:, 0, :].squeeze().tolist()  # å–å¾— [CLS] å‘é‡
    embedding_2 = output_2.last_hidden_state[:, 0, :].squeeze().tolist()  # å–å¾— [CLS] å‘é‡
    return embedding_1 + embedding_2  # æ‹¼æ¥å…©å€‹å‘é‡

# é€£ç·š PostgreSQL
conn = psycopg2.connect(DATABASE_URL)
cur = conn.cursor()

# æ–°å¢é™¤éŒ¯è¨Šæ¯

def search_similar_blocks(query):
    # è½‰æ›æœå°‹è©ç‚ºå‘é‡
    query_vector = text_to_embedding(query)
    print(f"{'='*20}")  # é™¤éŒ¯è¨Šæ¯
    print(f"Query Vector: {np.array(query_vector)}")  # é™¤éŒ¯è¨Šæ¯

    # ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æœå°‹æœ€ç›¸é—œçš„å½±ç‰‡ç‰‡æ®µ
    cur.execute("""
        SELECT id, block_text, (embedding <-> %s::vector) AS distance
        FROM video_blocks
        ORDER BY distance ASC
        LIMIT 10;
    """, (query_vector,))
    # é¡¯ç¤ºæœå°‹çµæœ
    results = cur.fetchall()
    for id, block_text, distance in results:
        print(f"\nğŸ¥ {id} \nğŸ“œ {block_text} \nğŸ“ ç›¸ä¼¼åº¦: {distance:.4f}")
        # print(f"Embedding: {np.array(query_vector)}")  # é™¤éŒ¯è¨Šæ¯

try:
    while True:
        # æ¥å—ä½¿ç”¨è€…è¼¸å…¥çš„é—œéµè©
        query = input("è«‹è¼¸å…¥é—œéµè©ï¼ˆæˆ–è¼¸å…¥ 'exit' çµæŸï¼‰ï¼š")
        if query.lower() == 'exit':
            break
        search_similar_blocks(query)
except KeyboardInterrupt:
    print("\nå·²é€€å‡ºæœå°‹ã€‚")
finally:
    cur.close()
    conn.close()
